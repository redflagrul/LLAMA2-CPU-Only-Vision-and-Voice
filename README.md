LLAMA2 CPU-Only Vision and Voice Project
This project integrates LLAMA2 for text generation, PlayHT for voice synthesis, and OpenCV for webcam functionality, all running on the CPU. This guide helps you set up everything step-by-step.

Features:
LLAMA2 text generation on the CPU.
Voice synthesis via PlayHT API.
Basic webcam integration using OpenCV.
Table of Contents
Requirements
Installation
Setup
Running the Project
Project Structure
Scripts
Troubleshooting
Requirements
Python 3.10.6 or later
Git
PlayHT API Key (for voice cloning)
A GGUF model (3B parameters recommended) for LLAMA2 optimized for CPU usage
Installation
1. Clone the Repository
Open your terminal (or command prompt) and run the following command to clone this repository:

bash
Copy code
git clone https://github.com/yourusername/LLAMA2_CPU_Project.git
Navigate into the project folder:

bash
Copy code
cd LLAMA2_CPU_Project
2. Download a LLAMA2 GGUF Model
Go to Hugging Face and download a GGUF version of the LLAMA2 model optimized for CPU.
Place the GGUF model file in the LLAMA2_CPU_Project/llama2-webui/models/ folder.
3. Install Dependencies
Install the required libraries by running:

bash
Copy code
pip install requests transformers openai opencv-python sounddevice scipy
Next, install LLAMA2 WebUI dependencies:

bash
Copy code
cd llama2-webui
pip install -r requirements.txt
Setup
1. Configure LLAMA2 for CPU-Only Mode
In the llama2-webui folder, open config.py.

Set the model path to the GGUF model file you downloaded, and ensure the device is set to "cpu":

python
Copy code
model_path = "./models/your-llama2-gguf-model.gguf"  # replace with your actual model file
device = "cpu"
2. Configure PlayHT for Voice Responses
In the playht_voice_cpu.py file (provided below), update the following:
API Key: Replace "YOUR_PLAYHT_API_KEY" with your actual PlayHT API key.
Voice Model Name: Replace "YOUR_VOICE_MODEL_NAME" with your desired voice model name.
3. Test the PlayHT Script
Run playht_voice_cpu.py to ensure that PlayHT is working correctly.

bash
Copy code
python playht_voice_cpu.py
Running the Project
1. Run LLAMA2 WebUI
Start the LLAMA2 WebUI by running:

bash
Copy code
cd llama2-webui
python app.py
2. Run the Vision and Voice Integration Script
In a new terminal window, go back to the main project folder and run:

bash
Copy code
python vision_llama_cpu.py
This will open your webcam and start generating LLAMA2 responses, which are then converted to audio using PlayHT.

Project Structure
Your project folder should look like this:

graphql
Copy code
LLAMA2_CPU_Project/
├── llama2-webui/                  # LLAMA2 WebUI folder
│   ├── app.py
│   ├── config.py
│   ├── models/
│   │   └── your-llama2-gguf-model.gguf  # The GGUF model you downloaded
│   └── requirements.txt
├── playht_voice_cpu.py            # PlayHT voice synthesis script
└── vision_llama_cpu.py            # Main vision, LLAMA2, and voice integration script
Scripts
playht_voice_cpu.py
This script integrates with PlayHT to generate voice from the text generated by LLAMA2.

python
Copy code
import requests
import json
import time

# Replace with your PlayHT API Key
PLAYHT_API_KEY = "YOUR_PLAYHT_API_KEY"  # Replace with your actual API key
VOICE_ID = "YOUR_VOICE_MODEL_NAME"  # Replace with your voice model name

def generate_audio(text):
    url = "https://play.ht/api/v1/convert"
    headers = {
        "Authorization": f"Bearer {PLAYHT_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "voice": VOICE_ID,
        "content": text,
        "lang": "en",
        "speed": 1.0,
        "pitch": 1.0
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        audio_url = response.json().get("audio_url")
        print(f"Audio URL: {audio_url}")
        audio_response = requests.get(audio_url)

        with open("output.mp3", "wb") as f:
            f.write(audio_response.content)
        
        print("Audio saved as 'output.mp3'")
    else:
        print(f"Error: {response.status_code}")
        print(response.text)

if __name__ == "__main__":
    # Example text to generate voice for
    generate_audio("Hello, this is a test message from LLAMA2.")
vision_llama_cpu.py
This script integrates the vision capabilities with LLAMA2 and PlayHT. It captures webcam input and generates voice responses from LLAMA2.

python
Copy code
import cv2
import time
from transformers import LlamaForCausalLM, LlamaTokenizer
import torch
import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import os

# Load the LLAMA2 model (assuming it's GGUF format)
model_path = "./models/your-llama2-gguf-model.gguf"  # Replace with your actual GGUF model path
device = "cpu"

# Load the model
tokenizer = LlamaTokenizer.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(model_path)
model.to(device)

def generate_response(input_text):
    inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
    outputs = model.generate(inputs, max_length=100, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Initialize webcam
cap = cv2.VideoCapture(0)

# Set up a loop to capture video frames and run LLAMA2 + PlayHT
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Display the webcam feed
    cv2.imshow("Webcam Feed", frame)
    
    # Every 5 seconds, generate a response from LLAMA2
    if time.time() % 5 < 1:
        input_text = "Hello, what can I do for you today?"
        print(f"Input: {input_text}")
        
        response = generate_response(input_text)
        print(f"LLAMA2 Response: {response}")
        
        # Call PlayHT to generate voice (using playht_voice_cpu.py)
        generate_audio(response)  # Use function from playht_voice_cpu.py
        
    # Exit if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
Troubleshooting
PlayHT Errors: Ensure your API key and voice model name are correctly entered in playht_voice_cpu.py.
LLAMA2 Model Errors: Double-check the model path in config.py and ensure it’s the correct GGUF model.
Webcam Issues: Ensure opencv-python is installed correctly. You can test webcam functionality with a basic OpenCV script to capture and display video.
This setup guide should help you get LLAMA2, PlayHT, and webcam integration running on a CPU-only machine. Let me know if you have any issues!
