# LLAMA2 CPU-Only Vision and Voice Project 🧠🎤📷

Welcome to the **LLAMA2 CPU-Only Vision and Voice Project**! This project combines **LLAMA2** for text generation, **PlayHT** for voice synthesis, and **OpenCV** for webcam functionality, all powered by your CPU. 🚀

---

## Features ✨
- 💻 **LLAMA2** text generation on CPU.
- 🎙️ **PlayHT** voice synthesis for text-to-speech.
- 📸 **OpenCV** webcam integration.

---

## Table of Contents 📑

1. [Requirements](#requirements)
2. [Installation](#installation)
3. [Setup](#setup)
4. [Running the Project](#running-the-project)
5. [Scripts](#scripts)
6. [Troubleshooting](#troubleshooting)

---

## Requirements 📋

Before getting started, you'll need the following:

- **Python 3.10.6** or later 🐍
- **Git** for version control 🧑‍💻
- **PlayHT API Key** (for voice cloning) 🎧
- **LLAMA2 GGUF model** (3B parameters recommended) 🧠

---

## Installation 🛠️

### 1. Clone the Repository 🚀

First, clone the repository to your local machine:

```bash
git clone https://github.com/yourusername/LLAMA2_CPU_Project.git
cd LLAMA2_CPU_Project

2. Download the LLAMA2 GGUF Model 🧠
Go to Hugging Face and download a GGUF version of the LLAMA2 model optimized for CPU.
Place the GGUF model file in the LLAMA2_CPU_Project/llama2-webui/models/ folder.
3. Install Dependencies 📦
Install the required Python libraries by running:

pip install requests transformers openai opencv-python sounddevice scipy

cd llama2-webui
pip install -r requirements.txt

Setup ⚙️
1. Configure LLAMA2 for CPU-Only Mode 🧠
In the llama2-webui folder, open config.py.
Set the model path to your downloaded GGUF model file, and ensure the device is set to "cpu":

model_path = "./models/your-llama2-gguf-model.gguf"  # Replace with your actual model file
device = "cpu"

model_path = "./models/your-llama2-gguf-model.gguf"  # Replace with your actual model file
device = "cpu"
model_path = "./models/your-llama2-gguf-model.gguf"  # Replace with your actual model file
device = "cpu"
2. Configure PlayHT for Voice Responses 🎧
Open the playht_voice_cpu.py file (provided below).
Update these variables:
API Key: Replace "YOUR_PLAYHT_API_KEY" with your PlayHT API key.
Voice Model Name: Replace "YOUR_VOICE_MODEL_NAME" with the desired PlayHT voice model.
3. Test PlayHT Script 🧑‍💻
Run the playht_voice_cpu.py script to ensure PlayHT is working properly:

python playht_voice_cpu.py

Running the Project 🏃‍♂️
1. Run LLAMA2 WebUI 🧠
To start the LLAMA2 WebUI, run:

cd llama2-webui
python app.py

2. Run the Vision and Voice Integration Script 📷🎙️
In a new terminal window, run:

python vision_llama_cpu.py

This will open your webcam and generate LLAMA2 responses, which will be converted into speech by PlayHT.

Scripts 📜
playht_voice_cpu.py 🎤
This script generates voice from the text generated by LLAMA2 using PlayHT.

import requests
import json
import time

# Replace with your PlayHT API Key
PLAYHT_API_KEY = "YOUR_PLAYHT_API_KEY"  # Replace with your actual API key
VOICE_ID = "YOUR_VOICE_MODEL_NAME"  # Replace with your voice model name

def generate_audio(text):
    url = "https://play.ht/api/v1/convert"
    headers = {
        "Authorization": f"Bearer {PLAYHT_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "voice": VOICE_ID,
        "content": text,
        "lang": "en",
        "speed": 1.0,
        "pitch": 1.0
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        audio_url = response.json().get("audio_url")
        print(f"Audio URL: {audio_url}")
        audio_response = requests.get(audio_url)

        with open("output.mp3", "wb") as f:
            f.write(audio_response.content)
        
        print("Audio saved as 'output.mp3'")
    else:
        print(f"Error: {response.status_code}")
        print(response.text)

if __name__ == "__main__":
    # Example text to generate voice for
    generate_audio("Hello, this is a test message from LLAMA2.")

vision_llama_cpu.py 📸
This script captures webcam input and generates voice responses from LLAMA2.

import cv2
import time
from transformers import LlamaForCausalLM, LlamaTokenizer
import torch
import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import os

# Load the LLAMA2 model (assuming it's GGUF format)
model_path = "./models/your-llama2-gguf-model.gguf"  # Replace with your actual GGUF model path
device = "cpu"

# Load the model
tokenizer = LlamaTokenizer.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(model_path)
model.to(device)

def generate_response(input_text):
    inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
    outputs = model.generate(inputs, max_length=100, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Initialize webcam
cap = cv2.VideoCapture(0)

# Set up a loop to capture video frames and run LLAMA2 + PlayHT
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Display the webcam feed
    cv2.imshow("Webcam Feed", frame)
    
    # Every 5 seconds, generate a response from LLAMA2
    if time.time() % 5 < 1:
        input_text = "Hello, what can I do for you today?"
        print(f"Input: {input_text}")
        
        response = generate_response(input_text)
        print(f"LLAMA2 Response: {response}")
        
        # Call PlayHT to generate voice (using playht_voice_cpu.py)
        generate_audio(response)  # Use function from playht_voice_cpu.py
        
    # Exit if 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()


Troubleshooting ⚠️
PlayHT Errors: Ensure your API key and voice model name are correctly entered in playht_voice_cpu.py.
LLAMA2 Model Errors: Double-check the model path in config.py and ensure it’s the correct GGUF model.
Webcam Issues: Ensure opencv-python is installed correctly. You can test webcam functionality with a basic OpenCV script to capture and display video.
Enjoy! 🎉
You're now ready to run your LLAMA2 CPU-Only Vision and Voice Project. Enjoy the integration of vision, voice, and text generation! 🚀

Feel free to open issues or contribute to the project. Let's build something amazing! 💡
