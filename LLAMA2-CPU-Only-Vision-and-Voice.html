<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLAMA2 CPU-Only Vision and Voice Project Guide</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 20px;
            background-color: #f4f4f4;
            color: #333;
        }
        h1, h2 {
            color: #333;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
        }
        code {
            background-color: #f5f5f5;
            padding: 5px;
            border-radius: 3px;
        }
        .important {
            color: red;
            font-weight: bold;
        }
    </style>
</head>
<body>

    <h1>LLAMA2 CPU-Only Vision and Voice Project Guide</h1>
    <p>This project integrates **LLAMA2** for text generation, **PlayHT** for voice synthesis, and **OpenCV** for webcam functionality, all running on the CPU. This guide will help you set up everything step-by-step.</p>

    <h2>Features:</h2>
    <ul>
        <li>LLAMA2 text generation on the CPU.</li>
        <li>Voice synthesis via PlayHT API.</li>
        <li>Basic webcam integration using OpenCV.</li>
    </ul>

    <h2>Table of Contents</h2>
    <ul>
        <li><a href="#requirements">Requirements</a></li>
        <li><a href="#installation">Installation</a></li>
        <li><a href="#setup">Setup</a></li>
        <li><a href="#running">Running the Project</a></li>
        <li><a href="#scripts">Scripts</a></li>
        <li><a href="#troubleshooting">Troubleshooting</a></li>
    </ul>

    <h2 id="requirements">1. Requirements</h2>
    <ul>
        <li><strong>Python 3.10.6</strong> or later</li>
        <li><strong>Git</strong></li>
        <li>PlayHT API Key (for voice cloning)</li>
        <li>A GGUF model (3B parameters recommended) for LLAMA2 optimized for CPU usage</li>
    </ul>

    <h2 id="installation">2. Installation</h2>
    <p>Follow these steps to install the necessary tools and dependencies:</p>

    <h3>Clone the Repository</h3>
    <p>Open your terminal (or command prompt) and run the following command to clone the repository:</p>
    <pre><code>git clone https://github.com/yourusername/LLAMA2_CPU_Project.git</code></pre>
    <p>Navigate into the project folder:</p>
    <pre><code>cd LLAMA2_CPU_Project</code></pre>

    <h3>Download a LLAMA2 GGUF Model</h3>
    <p>1. Go to <a href="https://huggingface.co/">Hugging Face</a> and download a GGUF version of the LLAMA2 model optimized for CPU.</p>
    <p>2. Place the GGUF model file in the <code>LLAMA2_CPU_Project/llama2-webui/models/</code> folder.</p>

    <h3>Install Dependencies</h3>
    <p>Run the following commands to install the required libraries:</p>
    <pre><code>pip install requests transformers openai opencv-python sounddevice scipy</code></pre>
    <p>Next, install LLAMA2 WebUI dependencies:</p>
    <pre><code>cd llama2-webui
pip install -r requirements.txt</code></pre>

    <h2 id="setup">3. Setup</h2>
    <h3>Configure LLAMA2 for CPU-Only Mode</h3>
    <p>1. In the <code>llama2-webui</code> folder, open <code>config.py</code>.</p>
    <p>2. Set the model path to the GGUF model file you downloaded, and ensure the device is set to <code>"cpu"</code>:</p>
    <pre><code>model_path = "./models/your-llama2-gguf-model.gguf"  # replace with your actual model file
device = "cpu"</code></pre>

    <h3>Configure PlayHT for Voice Responses</h3>
    <p>1. Open <code>playht_voice_cpu.py</code> (provided in the scripts section below).</p>
    <p>2. Update the following:</p>
    <ul>
        <li>**API Key**: Replace <code>"YOUR_PLAYHT_API_KEY"</code> with your actual PlayHT API key.</li>
        <li>**Voice Model Name**: Replace <code>"YOUR_VOICE_MODEL_NAME"</code> with your desired voice model name.</li>
    </ul>

    <h3>Test the PlayHT Script</h3>
    <p>Run <code>playht_voice_cpu.py</code> to ensure that PlayHT is working correctly:</p>
    <pre><code>python playht_voice_cpu.py</code></pre>

    <h2 id="running">4. Running the Project</h2>
    <h3>Run LLAMA2 WebUI</h3>
    <p>Start the LLAMA2 WebUI by running:</p>
    <pre><code>cd llama2-webui
python app.py</code></pre>

    <h3>Run the Vision and Voice Integration Script</h3>
    <p>In a new terminal window, go back to the main project folder and run:</p>
    <pre><code>python vision_llama_cpu.py</code></pre>
    <p>This will open your webcam and start generating LLAMA2 responses, which are then converted to audio using PlayHT.</p>

    <h2 id="scripts">5. Scripts</h2>

    <h3>playht_voice_cpu.py</h3>
    <p>This script integrates with PlayHT to generate voice from the text generated by LLAMA2:</p>
    <pre><code>
import requests
import json
import time

# Replace with your PlayHT API Key
PLAYHT_API_KEY = "YOUR_PLAYHT_API_KEY"  # Replace with your actual API key
VOICE_ID = "YOUR_VOICE_MODEL_NAME"  # Replace with your voice model name

def generate_audio(text):
    url = "https://play.ht/api/v1/convert"
    headers = {
        "Authorization": f"Bearer {PLAYHT_API_KEY}",
        "Content-Type": "application/json"
    }
    data = {
        "voice": VOICE_ID,
        "content": text,
        "lang": "en",
        "speed": 1.0,
        "pitch": 1.0
    }

    response = requests.post(url, headers=headers, data=json.dumps(data))

    if response.status_code == 200:
        audio_url = response.json().get("audio_url")
        print(f"Audio URL: {audio_url}")
        audio_response = requests.get(audio_url)

        with open("output.mp3", "wb") as f:
            f.write(audio_response.content)
        
        print("Audio saved as 'output.mp3'")
    else:
        print(f"Error: {response.status_code}")
        print(response.text)

if __name__ == "__main__":
    # Example text to generate voice for
    generate_audio("Hello, this is a test message from LLAMA2.")
    </code></pre>

    <h3>vision_llama_cpu.py</h3>
    <p>This script integrates the vision capabilities with LLAMA2 and PlayHT. It captures webcam input and generates voice responses from LLAMA2:</p>
    <pre><code>
import cv2
import time
from transformers import LlamaForCausalLM, LlamaTokenizer
import torch
import sounddevice as sd
import numpy as np
import scipy.io.wavfile as wav
import os

# Load the LLAMA2 model (assuming it's GGUF format)
model_path = "./models/your-llama2-gguf-model.gguf"  # Replace with your actual GGUF model path
device = "cpu"

# Load the model
tokenizer = LlamaTokenizer.from_pretrained(model_path)
model = LlamaForCausalLM.from_pretrained(model_path)
model.to(device)

def generate_response(input_text):
    inputs = tokenizer.encode(input_text, return_tensors="pt").to(device)
    outputs = model.generate(inputs, max_length=100, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Initialize webcam
cap = cv2.VideoCapture(0)

# Set up a loop to capture video frames and run LLAMA2 + PlayHT
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    # Display the webcam feed
    cv2.imshow("Webcam Feed",
